{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929e75e7",
   "metadata": {},
   "source": [
    "# Cost projecting with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c2c2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pprint\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98684281",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2df81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key for making OpenAI requests\n",
    "# Similar to above, the key is fetched from the environment variables\n",
    "# 'openai' is a Python client library for accessing OpenAI's APIs, so this line sets the authentication to be used for all subsequent requests through 'openai'\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the Hugging Face token for making Hugging Face API calls\n",
    "# Once again, the token is fetched from the environment variables\n",
    "# 'HF_TOKEN' will now hold the token to be used for any calls to the Hugging Face API\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c355a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data, indent=0):\n",
    "    pp = pprint.PrettyPrinter(indent=indent)\n",
    "    for item in data:\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(\" \" * indent + f\"{key}:\")\n",
    "                print(value, indent + 4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3494cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for prices\n",
    "prices = {\n",
    "    'gpt-3.5-turbo': {\n",
    "        'input': 0.0015 / 1000,  # per token\n",
    "        'output': 0.002 / 1000  # per token\n",
    "    },\n",
    "    'ada': 0.0016 / 1000  # per token\n",
    "}\n",
    "\n",
    "def calculate_ada_cost(res):\n",
    "    # Calculate the cost\n",
    "    input_tokens = res['usage']['prompt_tokens']\n",
    "    output_tokens = res['usage']['completion_tokens']\n",
    "    total_tokens = res['usage']['total_tokens']\n",
    "    total_cost = total_tokens * prices['ada']\n",
    "    return {'input_tokens': input_tokens, 'output_tokens': output_tokens, 'total_cost': total_cost}\n",
    "\n",
    "\n",
    "def generate_openai_response(prompt, model='gpt-3.5-turbo', **kwargs):\n",
    "    if model in ('gpt-3.5-turbo', 'gpt-4'):\n",
    "        pretty_print(prompt)\n",
    "\n",
    "        chat_completion = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Calculate the cost\n",
    "        input_tokens = chat_completion['usage']['prompt_tokens']\n",
    "        output_tokens = chat_completion['usage']['completion_tokens']\n",
    "        input_cost = input_tokens * prices[model]['input']\n",
    "        output_cost = output_tokens * prices[model]['output']\n",
    "        total_cost = input_cost + output_cost\n",
    "        return chat_completion, {'input_cost': input_cost, 'output_cost': output_cost, 'total_cost': total_cost}\n",
    "    elif 'ada' in model:\n",
    "\n",
    "        # Generate a completion using the fine-tuned model\n",
    "        res = openai.Completion.create(\n",
    "            model=model, \n",
    "            prompt=prompt,\n",
    "            **kwargs\n",
    "        )\n",
    "        return res, calculate_ada_cost(res)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c2f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the text we want to classify\n",
    "\n",
    "text = 'you are such a loser! I cannot believe you are even here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d7db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baff1ec2",
   "metadata": {},
   "source": [
    "# Cost projecting with OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91700683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<OpenAIObject chat.completion id=chatcmpl-8Hxh12Rn2td42h1LZz56WAqLpu50d at 0x11a707650> JSON: {\n",
       "   \"id\": \"chatcmpl-8Hxh12Rn2td42h1LZz56WAqLpu50d\",\n",
       "   \"object\": \"chat.completion\",\n",
       "   \"created\": 1699291727,\n",
       "   \"model\": \"gpt-3.5-turbo-0613\",\n",
       "   \"choices\": [\n",
       "     {\n",
       "       \"index\": 0,\n",
       "       \"message\": {\n",
       "         \"role\": \"assistant\",\n",
       "         \"content\": \"Reasoning: This text uses derogatory language and insults to demean and belittle the reader, calling them a \\\"loser.\\\" It demonstrates a disrespectful and offensive tone.\\nLabel: Toxic\"\n",
       "       },\n",
       "       \"finish_reason\": \"stop\"\n",
       "     }\n",
       "   ],\n",
       "   \"usage\": {\n",
       "     \"prompt_tokens\": 94,\n",
       "     \"completion_tokens\": 36,\n",
       "     \"total_tokens\": 130\n",
       "   }\n",
       " },\n",
       " {'input_cost': 0.000141,\n",
       "  'output_cost': 7.2e-05,\n",
       "  'total_cost': 0.00021300000000000003})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_openai_response(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'You are a bot that classifies whether a given piece of text is toxic or not. Use \"Toxic\" or \"Non-Toxic\"'}, \n",
    "        {'role': 'user', 'content': f'Use this reasoning:\\nText: (the input text)\\nReasoning: (an explanation of why the language is toxic)\\nLabel: (Either \"Non-Toxic\" or \"Toxic\")\\n\\nText: {text}'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b4fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e411123d",
   "metadata": {},
   "source": [
    "# Cost projecting with Huggingface Inference Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc24365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def call_huggingface_api(text, params=None):\n",
    "    url = \"https://d2q5h5r3a1pkorfp.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "    endpoint = \"/\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"inputs\": text,\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    json_data = json.dumps(data)\n",
    "\n",
    "    response = requests.post(url + endpoint, data=json_data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84b701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c32672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Toxic', 'score': 0.6368551254272461}, {'label': 'Non-Toxic', 'score': 0.3631448745727539}]\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"top_k\": None\n",
    "}\n",
    "\n",
    "result = call_huggingface_api(text, parameters)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a5ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d350f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211267.60563380283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it should take over 200k OpenAI calls to hit 40 dollars. Not too bad!\n",
    "45 / 0.000213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "79e785b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d2503a",
   "metadata": {},
   "source": [
    "# Cost projecting with OpenAI Fine-tuned model\n",
    "\n",
    "Predicting one token with a fine-tuned ada model is about the same as GPT-3.5 turbo.\n",
    "\n",
    "```\n",
    "prices = {\n",
    "    'gpt-3.5-turbo': {\n",
    "        'input': 0.0015 / 1000,  # per token\n",
    "        'output': 0.002 / 1000  # per token\n",
    "    },\n",
    "    'ada': 0.0016 / 1000  # per token\n",
    "}\n",
    "```\n",
    "\n",
    "Note that Ada is actually more expensive that gpt 3.5 input tokens so for short outputs, gpt-3.5 is cheaper than fine-tuning. For longer outputs, switching to ada might be more beneficial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55dfbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 57, 'output_tokens': 1, 'total_cost': 9.28e-05}\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Great pieces of jewelry for the price\n",
    "\n",
    "Great pieces of jewelry for the price. The 6mm is perfect for my tragus piercing. I gave four stars because I already lost one because it fell out! Other than that I am very happy with the purchase!\n",
    "\n",
    "###\n",
    "\n",
    "'''\n",
    "\n",
    "res, cost = generate_openai_response(prompt, model='ada:ft-personal-2023-05-08-16-25-48', \n",
    "                                     temperature=0, max_tokens=1, logprobs=3)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93652c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Star: 4\n",
      "Probabilities:\n",
      " 4: 0.9959\n",
      " 5: 0.0025\n",
      " 3: 0.0013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract logprobs from the API response\n",
    "probs = []\n",
    "logprobs = res.choices[0].logprobs.top_logprobs\n",
    "# Convert logprobs to probabilities and store them in the 'probs' list\n",
    "for logprob in logprobs:\n",
    "    _probs = {}\n",
    "    for key, value in logprob.items():\n",
    "        _probs[key] = math.exp(value)\n",
    "    probs.append(_probs)\n",
    "# Extract the predicted category (star) from the API response\n",
    "pred = res['choices'][0].text.strip()\n",
    "# Nicely print the prompt, predicted category, and probabilities\n",
    "print(\"Predicted Star:\", pred)\n",
    "print(\"Probabilities:\")\n",
    "for prob in probs:\n",
    "    for key, value in sorted(prob.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6045617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<OpenAIObject chat.completion id=chatcmpl-8HxhzsxieW4PrexsR3pyT5yEkwALa at 0x11a7077d0> JSON: {\n",
       "   \"id\": \"chatcmpl-8HxhzsxieW4PrexsR3pyT5yEkwALa\",\n",
       "   \"object\": \"chat.completion\",\n",
       "   \"created\": 1699291787,\n",
       "   \"model\": \"gpt-3.5-turbo-0613\",\n",
       "   \"choices\": [\n",
       "     {\n",
       "       \"index\": 0,\n",
       "       \"message\": {\n",
       "         \"role\": \"assistant\",\n",
       "         \"content\": \"Toxic\"\n",
       "       },\n",
       "       \"finish_reason\": \"stop\"\n",
       "     }\n",
       "   ],\n",
       "   \"usage\": {\n",
       "     \"prompt_tokens\": 58,\n",
       "     \"completion_tokens\": 2,\n",
       "     \"total_tokens\": 60\n",
       "   }\n",
       " },\n",
       " {'input_cost': 8.7e-05, 'output_cost': 4e-06, 'total_cost': 9.1e-05})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_openai_response([\n",
    "        {'role': 'system', 'content': 'You are a bot that classifies whether a given piece of text is toxic or not. Use \"Toxic\" or \"Non-Toxic\"'}, \n",
    "        {'role': 'user', 'content': f'Text: {text}\\nLabel:'}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73f7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a5597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efccf470",
   "metadata": {},
   "source": [
    "# Starting to Think About Evaluation of Models\n",
    "When evaluating the performance of a language model, it's important to consider a variety of factors. Apart from obvious ones like the quality of output, cost, and ease of use, another key factor is the latency of the model, i.e., the time it takes from when a request is made until the result is received.\n",
    "\n",
    "## Latency of an LLM\n",
    "Large Language Models (LLMs) can be resource-intensive. When running models like GPT-3 or BERT, you need substantial computational power, and generating results may take considerable time. The latency you experience can vary significantly depending on whether you're using your own infrastructure or a third-party API service like OpenAI.\n",
    "\n",
    "### API Calls\n",
    "When you use a third-party API like OpenAI, you're essentially renting time on their servers to perform your task. This can have a significant impact on the latency of the model. The travel time for the request and response data, the load on the servers, and the processing capabilities of the service all contribute to the overall latency.\n",
    "\n",
    "On the upside, such services often have powerful servers designed to handle these tasks efficiently, so latency can still be relatively low even with these additional factors. Moreover, they manage the infrastructure, so you don't have to worry about setup, maintenance, or scaling.\n",
    "\n",
    "### Running Models on Local Compute\n",
    "Running models on your own servers or local machine eliminates the data travel time, and you have direct control over the computational resources allocated to the task. However, the latency will largely depend on the processing power of your local machine or server. If the infrastructure is not as powerful as that of a dedicated API service, the latency may be higher.\n",
    "\n",
    "Additionally, running LLMs on your own compute means that you have to manage everything from setup to maintenance, which can add overhead not present when using an API service.\n",
    "\n",
    "### Comparison\n",
    "While third-party APIs can provide a powerful, easy-to-use solution with potentially low latency, running models on your own compute provides more control over the entire process. The best choice depends on your specific use case, considering factors such as latency requirements, computational resources, costs, and the overhead you're willing to manage.\n",
    "\n",
    "Remember, the lowest latency solution isn't always the best one if it compromises on other important aspects of your project. Always consider the trade-offs involved when choosing between these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c18fcdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 5.72 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# star rating with smaller fine-tuned model (about the same input/output as toxic vs non-toxic)\n",
    "%timeit generate_openai_response(prompt, model='ada:ft-personal-2023-05-08-16-25-48', temperature=0, max_tokens=1, logprobs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a86e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f4b7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 ms ± 50.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# no chain of thought toxic vs non toxic with OpenAI\n",
    "%timeit generate_openai_response([{'role': 'system', 'content': 'You are a bot that classifies whether a given piece of text is toxic or not. Use \"Toxic\" or \"Non-Toxic\"'}, {'role': 'user', 'content': f'Text: {text}\\nLabel:'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e151d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bebc204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 ms ± 52.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# toxic vs non toxic with custom classifier\n",
    "%timeit call_huggingface_api(text, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc356ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller fine tuned models are generally cheaper/faster than massive closed-source models like ChatGPT and GPT-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
